{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_CA4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPtvai1NtjB4",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XzTXe4rTbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed, randrange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from scipy.stats import mode\n",
        "from collections import Counter\n",
        "from statistics import mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvwQTha6uLq8",
        "colab_type": "text"
      },
      "source": [
        "## Divide Into Test And Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbL225mALemd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def divideTestAndTrain(dataset):\n",
        "  return train_test_split(dataset, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3EZ9ERk2dzf",
        "colab_type": "text"
      },
      "source": [
        "با استفاده از تابع بالا، دیتاست را تبدیل به دو دیتاست ترین و تست می‌کنیم که به صورت ۸۰-۲۰ تقسیم شده اند."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LN42QTLLm5W",
        "colab_type": "text"
      },
      "source": [
        "## Drop Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5HnppyMuOwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def droplabel(dataset, lbl):\n",
        "  labelFree = dataset.drop(lbl, axis=1)\n",
        "  label = dataset[lbl]\n",
        "  return labelFree, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMD-5-Wz2vjx",
        "colab_type": "text"
      },
      "source": [
        "در این تابع لیبل را از دیتاست جدا می‌کنیم. درواقع ستونی که میخواهیم پیشبینی کنیم را لیبل و بقیه‌ی فیچرها را لیبل‌فری می‌نامیم و جدا می‌کنیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0FvlsuRv4hw",
        "colab_type": "text"
      },
      "source": [
        "## Making Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHZ_8d8YNLqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeTree():\n",
        "  classifier = DecisionTreeClassifier()\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yix3LLB62_JS",
        "colab_type": "text"
      },
      "source": [
        "با استفاده از تابعی که در کتابخانه‌ها وجود دارد یک درخت تصمیم می‌سازیم و آن را برمی‌گردانیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTYSoegYNWKr",
        "colab_type": "text"
      },
      "source": [
        "## Fit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_CInY8uVYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fitting(classifier, labelFree_train, label_train):\n",
        "  classifier = classifier.fit(labelFree_train, label_train)\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9RP7cNn3eCE",
        "colab_type": "text"
      },
      "source": [
        "به تابع فیت، فیچرهای دیتاست ترین و لیبل دیتاست ترین را می‌دهیم و درخت را فیت می‌کنیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGu9lzZkwImc",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLCTR5h_v882",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(classifier, labelFree_test):\n",
        "  return classifier.predict(labelFree_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUU10N7z4J84",
        "colab_type": "text"
      },
      "source": [
        "با استفاده از تابع پردیکت، فیچرهای دیتاست تست را می‌دهیم و لیبل را پیش‌بینی می‌کنیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSqJlHDwgaF",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRMrJgHRwhWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAccuracy(label_test, label_pred):\n",
        "  return accuracy_score(label_test, label_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gel0dZq4gdE",
        "colab_type": "text"
      },
      "source": [
        "در این تابع می‌توانیم با استفاده از توابع موجود، اکیورسی (دقت) را محاسبه کنیم. این گونه که دیتاست پیش‌بینی شده و دیتاست لیبل اصلی را می‌دهیم و تابع، اکیورسی را خروجی می‌دهد."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnHG7fMrJd-P",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDwCtDJQJias",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DecisionTree(dataset, lbl):\n",
        "  train, test = divideTestAndTrain(dataset)\n",
        "\n",
        "  labelFree_train, label_train = droplabel(train, lbl)\n",
        "  labelFree_test, label_test = droplabel(test, lbl)\n",
        "  tree = makeTree()\n",
        "  tree = fitting(tree, labelFree_train, label_train)\n",
        "  pred = predict(tree, labelFree_test)\n",
        "  return getAccuracy(label_test, pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxeTm-UD4xwf",
        "colab_type": "text"
      },
      "source": [
        "در درخت تصمیم، ابتدا داده‌های ترین و تست را جدا می‌کنیم. برای هرکدام از داده‌های ترین و تست، لیبل و فیچرها را جدا می‌کنیم. سپس درخت را می‌سازیم و فیت می‌کنیم و پیشبینی می‌کنیم. در آخر هم دقت را برمی‌گردانیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si930vp4KgMz",
        "colab_type": "text"
      },
      "source": [
        "## Make Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIeRXlBEwmQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeTrees(dataset, treeNum, sampleNum):\n",
        "  trainDatas = []\n",
        "  for i in range(treeNum):\n",
        "    trainDatas.append(dataset.sample(n = sampleNum[i], replace=True))\n",
        "  return trainDatas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lkzxibU5mCt",
        "colab_type": "text"
      },
      "source": [
        "در این تابع، به اندازه‌ی ورودی، دیتاست را تقسیم می‌کنیم. تعداد درخت و سایز هر درخت را می‌دهیم و دیتاست داده شده را با جاگذاری تقسیم می‌کنیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soEgPoBf_5OJ",
        "colab_type": "text"
      },
      "source": [
        "## Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMtqpxh5GXSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vote(predictions):\n",
        "  return mode(predictions, axis = 0)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoCC2K-06tG0",
        "colab_type": "text"
      },
      "source": [
        "از جواب‌های پیشبینی تمام درخت‌ها را مود می‌گیرد و یک پیشبینی نهایی برمی‌گرداند. در واقع انگار که برای هر فیچر، رای‌گیری می‌کند و بیشترین تعداد برای یک فیچر خاص را، پیشبینی نهایی آن فیچر در نظر می‌گیرد."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqjlB3W7xzF",
        "colab_type": "text"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeKcX-UNKdyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Bagging(dataset, treeNum, sampleNum, lbl):\n",
        "  predictions= []\n",
        "\n",
        "  train, test = divideTestAndTrain(dataset)\n",
        "  labelFree_test, label_test = droplabel(test, lbl)\n",
        "\n",
        "  trainDatas = makeTrees(train, treeNum, sampleNum)\n",
        "\n",
        "  for i in trainDatas:\n",
        "    labelFree, label = droplabel(i, lbl)\n",
        "    tree = makeTree()\n",
        "    fittedTree = fitting(tree, labelFree, label)\n",
        "    pred = predict(fittedTree, labelFree_test)\n",
        "    predictions.append(pred)\n",
        "  \n",
        "  result = vote(predictions)\n",
        "  return getAccuracy(label_test, result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QM_874a7B6e",
        "colab_type": "text"
      },
      "source": [
        "در این قسمت، بعد از جدا کردن دیتاست تست و ترین، دیتاهای ترین را تبدیل به ۵ دسته‌ی ۱۵۰‌تایی می‌کنیم.\n",
        "سپس برای هر کدام از دسته‌ها یک درخت تصمیم می‌سازیم و فیت می ‌کنیم و پیشبینی می‌کنیم. در آخر هم رای گیری می‌کنیم و پیشبینی نهایی را برمی‌گردانیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGblZLC1KH7e",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckKUrvvfKKQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomForest(dataset, treeNum, sampleNum, lbl):\n",
        "  trainDatas = []\n",
        "  seed(time.time())\n",
        "  for i in range(treeNum):\n",
        "    sampleNum.append(randrange(0,len(dataset)))\n",
        "\n",
        "  train, test = divideTestAndTrain(dataset)\n",
        "  labelFree_test, label_test = droplabel(test, lbl)\n",
        "\n",
        "  trainDatas = makeTrees(train, treeNum, sampleNum)\n",
        "\n",
        "  seed(time.time())\n",
        "  predictions = []\n",
        "  for i in trainDatas:\n",
        "    featureNum = randrange(1,13)\n",
        "    # print(featureNum)\n",
        "    selectedFeatures = []\n",
        "    datasetTemp = i\n",
        "    j = 0\n",
        "    while(j < featureNum):\n",
        "    # for j in range(featureNum):\n",
        "      randFeature = datasetTemp.columns[randrange(0,len(datasetTemp.columns))]\n",
        "      if randFeature == lbl:\n",
        "        # print(j)\n",
        "        continue\n",
        "      selectedFeatures.append(randFeature)\n",
        "      datasetTemp = datasetTemp.drop(randFeature, axis=1)\n",
        "      j += 1\n",
        "    \n",
        "    selectedFeatures.append(lbl)\n",
        "    data = i[selectedFeatures]\n",
        "\n",
        "    labelFree, label = droplabel(data, lbl)\n",
        "    tree = makeTree()\n",
        "    fittedTree = fitting(tree, labelFree, label)\n",
        "    selectedFeatures.remove(lbl)\n",
        "    pred = predict(fittedTree, labelFree_test[selectedFeatures])\n",
        "    predictions.append(pred)\n",
        "\n",
        "  result = vote(predictions)\n",
        "  return getAccuracy(label_test, result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKMYBewe7595",
        "colab_type": "text"
      },
      "source": [
        "در جنگل رندوم، دیتاست اولیه را تبدیل به ترین و تست می‌کنیم. سپس دیتاست ترین را تبدیل به دسته‌هایی می‌کنیم که تعداد دسته‌ها رندوم است. سپس هرکدام از دسته‌ها را برمی‌داریم و تعدادی رندوم از فیچر‌هایش را نگه می‌داریم. سپس از روی این دیتاست‌هایی که هم تعداد سطرها و هم تعداد ستون‌هایشان فرق دارد و به صورت رندوم انتخاب شده است، درخت تصمیم می‌سازیم. سپس بعد از فیت کردن، پیشبینی می‌کنیم و رای می‌گیریم و پیش‌بینی نهایی را به دست می‌آوریم. سپس دقت را برمی‌گردانیم."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKgWikLHQhuq",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apjH-vOXQimF",
        "colab_type": "code",
        "outputId": "692b817f-2647-414e-e5f6-2c376fde4e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "treeNum = 5\n",
        "sampleNum = []\n",
        "lbl = 'target'\n",
        "dataset = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# 1\n",
        "print(\"2.1\")\n",
        "result = []\n",
        "for i in range(100):\n",
        "  acc = DecisionTree(dataset, lbl)\n",
        "  result.append(acc)\n",
        "result = np.array(result)\n",
        "print(\"Decision Tree: \", result.mean())\n",
        "\n",
        "# 2\n",
        "print(\"2.2\")\n",
        "for i in range(treeNum):\n",
        "  sampleNum.append(150)\n",
        "result = []\n",
        "for i in range(100):\n",
        "  acc = Bagging(dataset, treeNum, sampleNum, lbl)\n",
        "  result.append(acc)\n",
        "result = np.array(result)\n",
        "print(\"Bagging: \", result.mean())\n",
        "\n",
        "# 3\n",
        "print(\"2.3\")\n",
        "for col in dataset.columns:\n",
        "  result = []\n",
        "  if col == 'target':\n",
        "    continue\n",
        "  data = dataset.drop(col, axis=1)\n",
        "  for i in range(100):\n",
        "    acc = DecisionTree(data, lbl)\n",
        "    result.append(acc)\n",
        "  result = np.array(result)\n",
        "  print(col, result.mean())\n",
        "\n",
        "# 4\n",
        "print(\"2.4\")\n",
        "seed(time.time())\n",
        "selectedFeatures = []\n",
        "datasetTemp = dataset\n",
        "datasetTemp = datasetTemp.drop('target', axis=1)\n",
        "for i in range(5):\n",
        "  randFeature = datasetTemp.columns[randrange(0,len(datasetTemp.columns))]\n",
        "  selectedFeatures.append(randFeature)\n",
        "  datasetTemp = datasetTemp.drop(randFeature, axis=1)\n",
        "selectedFeatures.append('target')\n",
        "# print(dataset[selectedFeatures])\n",
        "print(DecisionTree(dataset[selectedFeatures], lbl))\n",
        "\n",
        "# 5\n",
        "print(\"2.5\")\n",
        "result = []\n",
        "for i in range(100):\n",
        "  acc = randomForest(dataset, treeNum, sampleNum, lbl)\n",
        "  result.append(acc)\n",
        "result = np.array(result)\n",
        "print(\"Random Forest: \", result.mean())\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1\n",
            "Decision Tree:  0.7496721311475407\n",
            "2.2\n",
            "Bagging:  0.777049180327869\n",
            "2.3\n",
            "age 0.7509836065573771\n",
            "sex 0.7368852459016394\n",
            "cp 0.7272131147540984\n",
            "trestbps 0.7596721311475408\n",
            "chol 0.7575409836065574\n",
            "fbs 0.7575409836065573\n",
            "restecg 0.7486885245901638\n",
            "thalach 0.761967213114754\n",
            "exang 0.7537704918032786\n",
            "oldpeak 0.7468852459016392\n",
            "slope 0.750327868852459\n",
            "ca 0.7180327868852459\n",
            "thal 0.7434426229508196\n",
            "2.4\n",
            "0.7213114754098361\n",
            "2.5\n",
            "Random Forest:  0.7678688524590165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKvZMCxCB7EJ",
        "colab_type": "text"
      },
      "source": [
        "1. همچنین کمک می‌کند که درخت اورفیت نشود. بیشتر اوقات روی درخت تصمیم پیاده‌سازی می‌شود. اما می‌توان روی الگوریتم‌های دیگر هم پیاده‌سازی کرد.\n",
        "بوت استرپینگ این گونه است که زمانی که داده‌های ترین را تقسیم می‌کنیم و می‌خواهیم تعدادی از داده‌ها را انتخاب کنیم، این کار را با جایگذاری انجام می‌دهیم. به این معنی که بعضی از داده‌ها می‌توانند دوبار در دیتا تکرار شوند. به دلیل رندوم بودن، واریانس و انحراف معیار را افزایش می‌دهد."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzP_CwryB8wM",
        "colab_type": "text"
      },
      "source": [
        "2. اورفیتینگ به این معنی است که پیشبینی نهایی ما خیلی به دیتاست ترین نزدیک شده باشد. یعنی پیچیدگی مسئله رو اینقدر بالا برده باشیم که داده‌ی ترین ما را خیلی خوب پیش‌بینی کند. اما اگر داده‌ی تست را به آن بدهیم، دقت آن پایین باشد. در واقع یادگیری ما بیش از اندازه فیت ترین دیتای ما شده است.\n",
        "درخت تصمیم به اورفیتینگ حساس است زیرا درواقع داده‌ی ترین را نمی‌بیند و یادگیری آن به صورت کامل از فیت کردن صورت می‌گیرد. بنابراین درخت تصمیم به اورفیتینگ حساس است.\n",
        "در بگینگ دیتاها را به دسته‌هایی تقسیم می‌کنیم. اینگونه، درخت‌ها به کل دیتا اورفیت نمی‌شوند. اما به بخشی از دیتا اورفیت ممکن از بشوند."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMqJA201B-tw",
        "colab_type": "text"
      },
      "source": [
        "3. رندوم فارست تلاش می‌کند که مسئله‌ی اورفیتینگی که در بگینگ رخ می‌دهد را حل کند.\n",
        "تفاوت بین این دو هم این است که در بگینگ تمام فیچرها را برای پیش بینی در نظر می‌گیریم. و فیچرهای درخت‌ها یکسان هستند که این ممکن است باعث ایجاد اورفیتینگ شود.\n",
        "اما در رندوم فارست برای اینکه مسئله‌ی اورفیت رخ ندهد و هر دفعه سعی  کنیم بهترین فیچر‌ها را انتخاب کند و اینکه در هر درخت فیچرهایی که انتخاب می‌کند متفاوت و به صورت رندوم است."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5lxiHekCAPj",
        "colab_type": "text"
      },
      "source": [
        "4. میانگین دقت سه روش درخت تصمیم و بگینگ و رندوم فارست به ترتیب:\n",
        "Decision Tree:  0.7491803278688525\n",
        "Bagging:  0.7786885245901638\n",
        "Random Forest:  0.7647540983606557\n",
        "از این دیتاها می‌توان فهمید که هرچه‌قدر درخت‌های بیشتر با فیچر‌های رندوم داشته باشیم، دقت مدل ما بالاتر می‌رود."
      ]
    }
  ]
}